
ipv4num = r"(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)" # From http://www.regular-expressions.info/regexbuddy/ipaccurate.html

ipv6num = "".join(map(str.strip, r"""(((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:)
            {6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0
            -4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1
            ,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d
            |[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})
            |((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2
            [0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]
            {1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)
            (\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}((
            (:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1
            \d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]
            {1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25
            [0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))
            |:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|
            2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))
            (%.+)?)""".split('\n'))) # Courtesy of Stephen Ryan at Dartware <http://forums.dartware.com/viewtopic.php?t=452>

def getStaticTLDs():
   """Return a static list of TLDs, generated by this module itself, ahead of time"""
   #[[[cog
   # import sys, os
   # sys.path.append(os.path.realpath(os.path.dirname(cog.inFile)))
   # import findurl
   # 
   # tlds = str(tuple(x for x in findurl.getTLDs()))
   #
   # cog.outl("return " + tlds)
   # 
   #]]]
   return ('AC', 'AD', 'AE', 'AERO', 'AF', 'AG', 'AI', 'AL', 'AM', 'AN', 'AO', 'AQ', 'AR', 'ARPA', 'AS', 'ASIA', 'AT', 'AU', 'AW', 'AX', 'AZ', 'BA', 'BB', 'BD', 'BE', 'BF', 'BG', 'BH', 'BI', 'BIZ', 'BJ', 'BM', 'BN', 'BO', 'BR', 'BS', 'BT', 'BV', 'BW', 'BY', 'BZ', 'CA', 'CAT', 'CC', 'CD', 'CF', 'CG', 'CH', 'CI', 'CK', 'CL', 'CM', 'CN', 'CO', 'COM', 'COOP', 'CR', 'CU', 'CV', 'CX', 'CY', 'CZ', 'DE', 'DJ', 'DK', 'DM', 'DO', 'DZ', 'EC', 'EDU', 'EE', 'EG', 'ER', 'ES', 'ET', 'EU', 'FI', 'FJ', 'FK', 'FM', 'FO', 'FR', 'GA', 'GB', 'GD', 'GE', 'GF', 'GG', 'GH', 'GI', 'GL', 'GM', 'GN', 'GOV', 'GP', 'GQ', 'GR', 'GS', 'GT', 'GU', 'GW', 'GY', 'HK', 'HM', 'HN', 'HR', 'HT', 'HU', 'ID', 'IE', 'IL', 'IM', 'IN', 'INFO', 'INT', 'IO', 'IQ', 'IR', 'IS', 'IT', 'JE', 'JM', 'JO', 'JOBS', 'JP', 'KE', 'KG', 'KH', 'KI', 'KM', 'KN', 'KP', 'KR', 'KW', 'KY', 'KZ', 'LA', 'LB', 'LC', 'LI', 'LK', 'LR', 'LS', 'LT', 'LU', 'LV', 'LY', 'MA', 'MC', 'MD', 'ME', 'MG', 'MH', 'MIL', 'MK', 'ML', 'MM', 'MN', 'MO', 'MOBI', 'MP', 'MQ', 'MR', 'MS', 'MT', 'MU', 'MUSEUM', 'MV', 'MW', 'MX', 'MY', 'MZ', 'NA', 'NAME', 'NC', 'NE', 'NET', 'NF', 'NG', 'NI', 'NL', 'NO', 'NP', 'NR', 'NU', 'NZ', 'OM', 'ORG', 'PA', 'PE', 'PF', 'PG', 'PH', 'PK', 'PL', 'PM', 'PN', 'PR', 'PRO', 'PS', 'PT', 'PW', 'PY', 'QA', 'RE', 'RO', 'RS', 'RU', 'RW', 'SA', 'SB', 'SC', 'SD', 'SE', 'SG', 'SH', 'SI', 'SJ', 'SK', 'SL', 'SM', 'SN', 'SO', 'SR', 'ST', 'SU', 'SV', 'SY', 'SZ', 'TC', 'TD', 'TEL', 'TF', 'TG', 'TH', 'TJ', 'TK', 'TL', 'TM', 'TN', 'TO', 'TP', 'TR', 'TRAVEL', 'TT', 'TV', 'TW', 'TZ', 'UA', 'UG', 'UK', 'US', 'UY', 'UZ', 'VA', 'VC', 'VE', 'VG', 'VI', 'VN', 'VU', 'WF', 'WS', 'XXX', 'YE', 'YT', 'ZA', 'ZM', 'ZW')
   #[[[end]]]

def generateRegex():
   # Join everything into a regex-compatible string

   tlds = getTLDs()
   tldreg = "("+"|".join(tlds)+")"

   schemes = getSchemes()
   schmreg = "("+"|".join(schemes)+")"

   validbody = r"[a-zA-Z0-9\-\.]"
   validstart = r"[a-zA-Z]"
   validmeat = "[^\s]"
   meat = r"(%s(\:%s)?@)?(%s%s*\.)+" % (validbody, validbody, validstart, validbody) + tldreg + "(/" + validmeat + "*)?(#" + validmeat + "*)?"

   withoutscheme = "("+meat+")" 

   withscheme = schmreg+r"://"+meat 
   withscheme = "("+withscheme+")"

   return withscheme


def getTLDs():
   """Scrape the list of TLDs from IANA"""

   import urllib, itertools

   tlds = iter(urllib.urlopen("http://data.iana.org/TLD/tlds-alpha-by-domain.txt").read().replace('\r\n', '\n').split('\n'))
   
   # Filter out all punycode domains, since that's not supported yet
   tlds = itertools.ifilter(lambda x: x.find('--') == -1, tlds)

   # Kill comments
   tlds = itertools.imap(lambda x: x.split("#", 1)[0], tlds)

   # Kill surrounding whitespace
   tlds = itertools.imap(str.strip, tlds)

   # Kill blank lines
   tlds = itertools.ifilter(lambda x: x != '', tlds)

   return tlds

def getSchemes():
    """Scrape a list of URL schemes from Wikipedia."""

    # Too much work for now, just return a static tuple

    return ('aaa', 
            'aaas', 
            'acap', 
            'cap', 
            'cid', 
            'crid', 
            'data', 
            'dav', 
            'dict', 
            'dns', 
            'fax', 
            'file', 
            'ftp', 
            'geo', 
            'go', 
            'gopher', 
            'h323', 
            'http', 
            'https', 
            'iax', 
            'icap', 
            'im', 
            'imap', 
            'info', 
            'ipp', 
            'iris', 
            'iris.beep', 
            'iris.xpc', 
            'iris.xpcs', 
            'iris.lws', 
            'ldap', 
            'lsid', 
            'mailto', 
            'mid', 
            'modem', 
            'msrp', 
            'msrps', 
            'mtqp', 
            'mupdate', 
            'news', 
            'nfs', 
            'nntp', 
            'opaquelocktoken', 
            'pop', 
            'pres', 
            'prospero', 
            'rsync', 
            'rtsp', 
            'service', 
            'shttp', 
            'sieve', 
            'sip', 
            'sips', 
            'sms', 
            'snmp', 
            'soap.beep', 
            'soap.beeps', 
            'tag', 
            'tel', 
            'telnet', 
            'tftp', 
            'thismessage', 
            'tip', 
            'tv', 
            'urn', 
            'vemmi', 
            'wais', 
            'xmlrpc.beep', 
            'xmlrpc.beeps', 
            'xmpp', 
            'z39.50r', 
            'z39.50s', 
            'about', 
            'adiumxtra', 
            'aim', 
            'apt', 
            'afp', 
            'aw', 
            'bitcoin', 
            'bolo', 
            'callto', 
            'chrome', 
            'coap', 
            'content', 
            'cvs', 
            'doi', 
            'ed2k', 
            'facetime', 
            'feed', 
            'finger', 
            'fish', 
            'git', 
            'gg', 
            'gizmoproject', 
            'gtalk', 
            'irc', 
            'ircs', 
            'irc6', 
            'itms', 
            'jar', 
            'javascript', 
            'keyparc', 
            'lastfm', 
            'ldaps', 
            'magnet', 
            'maps', 
            'mms', 
            'msnim', 
            'mumble', 
            'mvn', 
            'notes', 
            'palm', 
            'paparazzi', 
            'psyc', 
            'rmi', 
            'rtmp', 
            'secondlife', 
            'sgn', 
            'skype', 
            'spotify', 
            'ssh', 
            'sftp', 
            'smb', 
            'soldat', 
            'steam', 
            'svn', 
            'teamspeak', 
            'things', 
            'unreal', 
            'ut2004', 
            'ventrilo', 
            'view-source', 
            'webcal', 
            'ws', 
            'wss', 
            'wtai', 
            'wyciwyg', 
            'xfire', 
            'xri', 
            'ymsgr')

    #import urllib, itertools, xml.dom.minidom as xml

    #class FakeOpener(urllib.FancyURLopener):
    #    version = 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.2.17) Gecko/20110420 Firefox/3.6.17'

    #h = FakeOpener().open("http://en.wikipedia.org/wiki/Special:Export/URI_scheme").read()

    #d = xml.parseString(h).getElementsByTagName("page")[0].getElementsByTagName("revision")[0].getElementsByTagName("text")[0].firstChild.wholeText

    #return d

    def linkifyURLs(text):
        import re

        r = generateRegex()
        r = r"(?<=\b)" + r + r"(?=\b)" # Add some lookahead/lookbehind

        m=re.compile(r, flags=re.IGNORECASE)

        return re.sub(m, r'<a href="\1">\1</a>', text) # FIXME escape quotation marks in \1
